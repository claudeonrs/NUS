\documentclass{article}
\usepackage[a4paper, left=15mm, top=20mm, right=15mm,bottom=20mm]{geometry}
\usepackage{amsmath, amssymb, amsfonts}
\usepackage{fancyhdr}
\usepackage{graphicx}
\graphicspath{ {./images/} }
\usepackage{float}
\usepackage{hyperref}
\usepackage{lscape}
%\usepackage{arev}

\pagestyle{fancy}
\fancyhf{}
\lhead{ST2132}
\rhead{claudeonrs}
\rfoot{\thepage}
\usepackage{amsmath, amssymb, amsfonts, listings}
\usepackage{xcolor}
\usepackage{enumitem}
\setlist{nolistsep}


%New colors defined below
\definecolor{codegreen}{rgb}{0,0.6,0.4}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\definecolor{commentgreen}{rgb}{0.4,0.8,0.6}
%Code listing style named "mystyle"
\lstdefinestyle{mystyle}{
  backgroundcolor=\color{backcolour},
  commentstyle=\color{red},
  keywordstyle=\color{blue},
  numberstyle=\tiny\color{codegray},
  stringstyle=\color{codegreen},
  basicstyle=\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  captionpos=b,
  keepspaces=true,
  numbers=left,
  numbersep=5pt,
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2
}

%"mystyle" code listing set
\lstset{style=mystyle}

\title{No Title}
\author{Claudeon R Susanto}
\date{}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{lmodern}

\renewcommand{\familydefault}{\sfdefault}   % Supprime le serif (dyslexie)
\usepackage[font=sf, labelfont={sf}]{caption}
\usepackage{multicol}
\usepackage{makecell}
\renewcommand\theadalign{bc}
\renewcommand\theadfont{\bfseries}
\renewcommand\theadgape{\Gape[4pt]}
\renewcommand\cellgape{\Gape[4pt]}

\renewcommand\thesubsection{\thesection.\arabic{subsection}}
\setlength{\columnseprule}{1pt}
\begin{document}
%\maketitle
\fontfamily{lmss}\selectfont
\begin{multicols}{2}
\section{Probability Review}

\textbf{Multinomial Distribution}
$$\Pr(X_1=x_1, \dots, X_r = x_r) = {n \choose {x_1,\dots,x_r}} \prod_{i=1}^rp_i^{x_i}$$
\textbf{Mean Square Error (MSE)}
$$\text{E}\{(Y-c)^2\} = \text{var}(Y) + \{\text{E}(Y)-c\}^2$$
$$\text{E}\{(Y-c)^2|x\} = \text{var}[Y|x] + \{\text{E}[Y|x]-c\}^2$$
which are special cases of $\text{E}(Y^2) = \text{var}(Y) + [\text{E}(Y)]^2$. MSE is minimized if and only if $c=\text{E}(Y)$ or $\text{E}[Y|x]$.\\
Usually the formula for $\text{E}[Y|x]=f(x)$ is determined from observations/data and $x$ can be a vector of realisations from covariates.
$$\text{MSE}_{\text{empirical}} = \frac{1}{n}\sum_{i=1}^n\{\text{E}[Y|x_i]-y_i\}^2$$
In the real world, we have different realisations $x_i$ of the random variable $X$, hence the mean MSE is
$$\frac{1}{n}\sum_{i=1}^{n}\text{var}[Y|x_i] \approx \text{E}(\text{var}[Y|X]) \leq \text{var}(Y)$$
\textbf{Analysis of Variance (ANOVA)}\\
involves breaking of variance into components
$$\text{var}(Y) = \text{E}(\text{var}[Y|X]) + \text{var}(\text{E}[Y|X])$$

\subsection{Distributions}

\textbf{$\chi^2_1$ distribution}\\
Let $Z \sim \mathcal{N}(0,1)$. $V = Z^2$ has a $\chi^2$ distribution with 1 degree of freedom
$$f(v) = \frac{1}{\sqrt{2\pi}}v^{-1/2}e^{-v/2}$$

\textbf{Gamma distribution}
$$f(t) = \frac{\lambda^\alpha}{\Gamma(\alpha)}t^{\alpha-1}e^{-\lambda t}, t\geq 0$$
$$\Gamma(\alpha) = \int_{0}^{\infty}x^{\alpha-1}e^{-x}dx$$

\textbf{$\chi^2_n$ distribution}\\
Let $V_1,\dots,V_n$ be IID $\chi^2_1$
$$V = \sum_{i=1}^nV_i$$
has a $\chi^2_n$ distribution with $n$ degrees of freedom\\

\textbf{$t$ distribution}\\
Let $Z \sim \mathcal{N}(0,1)$ and $V \sim \chi^2_n$ be independent
$$t_n = \frac{Z}{\sqrt{V/n}}$$ has a $t$ distribution with $n$ degrees of freedom\\

\textbf{$F$ distribution}\\
Let $V \sim \chi_m^2$ and $W \sim \chi^2_n$ be independent
$$F_{m,n} = \frac{V/m}{W/n}$$ has an $F$ distribution with $(m,n)$ degrees of freedom\\
*Note: $t_n^2 = F_{1,n}$

\subsection{Sample Variance}
$$S^2 = \frac{1}{n-1}\sum_{i=1}^{n}(X_i-\bar{X})^2$$
$$\bar{X} \text{ and } S^2 \text{ are independent}$$
$$\bar{X} \sim \mathcal{N}\left(\mu,\frac{\sigma^2}{n}\right)$$
$$\frac{(n-1)S^2}{\sigma^2}\sim \chi^2_{n-1}$$
$$\frac{\bar{X}-\mu}{S/\sqrt{n}} \sim t_{n-1}$$
\section{Survey and Random Sampling}
Let $X_1, \dots, X_N$ be random draws without replacement from a population of size $N$ with mean $\mu$ and variance $\sigma^2$.
$$\text{cov}(X_i, X_j) = -\frac{\sigma^2}{N-1} \forall i\neq j$$
$$\text{var}(\bar{X}) = \left(\frac{N-n}{N-1}\right)\frac{\sigma^2}{n}$$

\subsection{Exchangeable}
RV's $Y_1, \dots, Y_k$ are exchangeable if all reordered vectors have the same distribution as $(Y_1, \dots Y_k)$. i.e. for any permutation $\pi$ on $\{1,\dots,K\}$,
$$(Y_{\pi(1)}, \dots, Y_{\pi(k)}) \stackrel{d}{=}(Y_{1}, \dots, Y_{k})$$
\subsection{Estimate and Estimator}
$$\bar{X} = \frac{1}{n}\sum_{i=1}^{n}X_i$$
$$\bar{x} = \frac{1}{n}\sum_{i=1}^{n}x_i$$
\begin{itemize}
	\item $\mu$, $\sigma$, $\sigma^2$ are \textbf{parameters}
	\item $\bar{x}$ is an \textbf{estimate} of $\mu$
	\item $\bar{x}$ is a realisation of the \textbf{estimator} $\bar{X}$
	\item \textbf{Standard Error (SE)} of the \underline{estimate (a number)} is defined as the SD of the estimator
	$$\text{SE} = \text{SD}(\bar{X}) = \frac{\sigma}{\sqrt{n}}$$
	which is how much $\bar{X}$ fluctuates around $\mu$ (a number) estimated from the data
	\item Estimate of $\sigma$
	\begin{itemize}
		\item Biased estimate of $\sigma^2$
		$$\hat{\sigma}^2 = \frac{1}{n}\sum_{i=1}^{n}(x_i-\bar{x})^2$$
		$$E(\hat{\sigma}^2) = \frac{n-1}{n}\sigma^2$$
		\item Unbiased estimate of $\sigma^2$ (preferred)
		$$s^2 = \frac{1}{n-1}\sum_{i=1}^{n}(x_i-\bar{x})^2$$
		$$E(s^2) = \sigma^2$$
	\end{itemize}

\end{itemize}
How to estimate $\mu$?
\begin{itemize}
	\item $\mu$ is estimated by $\bar{x}$
	\item Error in $\bar{x}$ is measured by the SE: $$\text{SD}(\bar{x})=\frac{\sigma}{\sqrt{n}}$$ which is \textbf{estimated} by $\frac{s}{\sqrt{n}}$ since $\sigma$ is unknown
	\item \textbf{Conclusion}: $\mu$ is estimated as $\bar{X}$, give or take $\frac{s}{\sqrt{n}}$
	$$ \text{ SE estimated by } \frac{s}{\sqrt{n}} = \frac{\sqrt{\frac{n}{n-1}}\times \text{SD} }{\sqrt{n}}$$
	$$\text{where SD } = \hat{\sigma}$$
\end{itemize}
How to estimate $p$?
\begin{itemize}
	\item $\hat{p}$ is the estimator of $p$
	$$E(\hat{p}) = p$$
	$$\text{var}(\hat{p}) = \frac{\sigma^2}{n} = \frac{p(1-p)}{n}$$
	$$\text{SE} = \text{SD}(\hat{p}) = \sqrt{\frac{p(1-p)}{n}}$$
	which is \textbf{estimated }by realisations of $\hat{p}$
\end{itemize}
\subsection{Interval estimation}
\subsubsection{Definitions}
\begin{itemize}
	\item For sufficiently large $n$,
	$$\frac{\bar{X}-\mu}{\sigma/\sqrt{n}} \sim \mathcal{N}(0,1)$$
	\item The $p$-quantile of $Z\sim\mathcal{N}(0,1)$ is the number $q$ such that
	$$\Phi(q) = \Pr(Z\leq q) = p$$
	$$q = \Phi^{-1}(p)$$
	\begin{lstlisting}[language=R]
q <- qnorm(p)
p <- pnorm(q)\end{lstlisting}
	\item For $0 < p < 0.5$, let $z_p$ be such that
	$$\Pr(Z > z_p) = p$$
	$$z_p = \Phi^{-1}(1-p)$$

	In other words, $z_p = (1-p)$-quantile of $Z$
\subsubsection{CI Estimation}
	\item For large $n$,
	$$\bar{X} \sim \mathcal{N}\left(\mu, \frac{\sigma^2}{n}\right)$$
	$$\Pr\left(-z_{\frac{\alpha}{2}} \leq \frac{\bar{X}-\mu}{\sigma/\sqrt{n}}\leq z_{\frac{\alpha}{2}} \right) \approx 1-\alpha$$

	$$\Pr\left( \bar{X} - z_{\frac{\alpha}{2}}\frac{\sigma}{\sqrt{n}} \leq \mu \leq \bar{X} + z_{\frac{\alpha}{2}}\frac{\sigma}{\sqrt{n}}\right) \approx 1-\alpha$$
	where the above, $\left( \bar{X} - z_{\frac{\alpha}{2}}\frac{\sigma}{\sqrt{n}} , \bar{X} + z_{\frac{\alpha}{2}}\frac{\sigma}{\sqrt{n}}\right)$ is a random interval. Realisation $\bar{x}$ of $\bar{X}$ gives the realised interval
	\item $(1-\alpha)$-CI for $\mu$ is of the form
	$$\left(\text{estimate} - z_{\frac{\alpha}{2}}\text{SE},\text{estimate} + z_{\frac{\alpha}{2}}\text{SE} \right)$$
\subsubsection{Exact CI}
	\item Let $t_{\frac{\alpha}{2}, n-1}$ be the number such that $$\Pr(t_{n-1} > t_{\frac{\alpha}{2}, n-1}) = \alpha/2$$
	\item \textbf{[Important]} Exact CI only works if $X \sim \mathcal{N}(\mu,\sigma^2)$ and $x_i$'s are realisations from IID \underline{Normal Distribution}\\
	* CI is exact means that $\Pr$($\mu$ is within the interval) is exactly $1-\alpha$
	\item $(1-\alpha)$-CI for $\mu$ is
	$$\left( \bar{x} - t_{\frac{\alpha}{2}, n-1}\frac{s}{\sqrt{n}},\bar{x} + t_{\frac{\alpha}{2}, n-1}\frac{s}{\sqrt{n}}\right)$$
\end{itemize}
\subsection{Bias in Survey}
Famous example: US presidential election survey conducted by \textit{Literary Digest} in 1936
\subsubsection{Bias in Measurement}
\begin{itemize}
	\item $x_1, \dots, x_n$ are realisations of random draws $X_i, \dots, X_n$ from a population with mean $\mu + b$ and variance $\sigma^2$
	\item SE $= \sigma/\sqrt{n}$ measures how far $\bar{x}$ is from $E(\bar{X}) = \mu + b$
	\item \textbf{Definition of Bias}
	$$\text{Bias of estimate} = \text{E}(\text{estimator}) - \text{parameter}$$
	\item MSE
	\begin{equation*}
		\begin{aligned}
			\text{E}(\bar{X}-\mu)^2 &= \text{var}(\bar{X}) + \{\text{E}(\bar{X}) - \mu\}^2\\
			\text{MSE} &= \text{SE}^2 + \text{bias}^2
		\end{aligned}
	\end{equation*}
However $\mu$ is unknowable, hence it is not possible to remove bias unless we make very careful observations
\end{itemize}

\section{Parameter Estimation}
Assuming data $x_1, \dots, x_n$ are realisations of IID RV's $X_1, \dots, X_n$ with density $f(x|\theta)$, estimate $\theta$.\\
The parameter $\theta$ lies in $\Theta \subseteq \mathbb{R}$ where $\Theta$ is the parameter space\\
How to estimate $\theta$ from realisations $x_1, \dots, x_n$?
\begin{enumerate}
	\item Method of moments
	\item Method of maximum likelihood
\end{enumerate}
\subsection{Method of moments}
Let $\hat{\theta}$ be an estimator for $\theta$.\\
The $k$-th moments of an RV $X$ is
$$\mu_k = E(X^k)$$
$$\frac{1}{n}\sum_{i=1}^{n}x_i^k$$
is a realisation of $\hat{\mu_k}$ and is used as estimate for $\mu_k$
$$\hat{\theta} = g(\hat{\mu_1}, \dots, \hat{\mu_q})$$
is an estimate for $\theta$ \textit{e.g.} for Normal RV,
$$g : \left[ \begin{matrix}
	x \\
	y
\end{matrix}\right] \rightarrow \left[ \begin{matrix}
x \\
y-x^2
\end{matrix}\right]$$
\subsection{Monte Carlo Approximation}
Needed if formula for $\theta$ is complicated/hard to compute the value of its expectation\\
\textbf{Rough Steps}:
\begin{enumerate}
	\item Estimate parameters $\theta$ using MOM/MLE
	\item Generate $n$ realisations $x_1, x_2, \dots, x_n$ using the estimated parameters and distribution
	\item From these $n$ realisations, estimate parameters again, these are realisations of $\hat{\theta}^*$
	\item Repeat steps 2 and 3 $m$ times until we get $m$ realisations of parameters $\theta$
	$$\text{SE} = \text{SD}(\hat{\theta}) \approx \text{SD}(\hat{\theta}^*)$$
	$$\text{Bias} = \text{E}(\hat{\theta}) - \theta \approx \text{E}(\hat{\theta^*}) - \theta_{\text{est.}}$$
	\item Finally, $\theta$ is around $\theta_{\text{est.}} -$ Bias $\pm$ SE, and the fitted distribution + parameter is called a \textbf{statistical model} for the event in question
\end{enumerate}
Note that as $n\rightarrow \infty$, $\text{E}(\hat{\theta}^*) \rightarrow \theta_{\text{est}}$ $\Rightarrow$ Bias$\rightarrow 0$, $\text{E}(\hat{\theta}) \rightarrow \theta$.
\begin{itemize}
	\item Thus, it is \textbf{asymptotically unbiased}
	\item Every MOM estimator is consistent, it goes to the parameter as $n \rightarrow \infty$
\end{itemize}

\subsection{Maximum Likelihood Method}
Let $x_1, \dots, x_n$ be realisations of IID RV's $X_1, \dots, X_n$ with density/mass function $f(x|\theta)$
$$L(\theta) = \prod_{i=1}^{n}f(x_i|\theta)$$
$$l(\theta) = \log{L(\theta)} = \sum_{i=1}^{n}\log f(x_i|\theta)$$
Find the value of $\theta$ that maximises the likelihood
\subsubsection{Multinomial Data}
$$L(p_1,\dots,p_r) = p_1^{x_1}\dots p_r^{x_r} \times c$$
$$l(p_1,\dots,p_r) = x_1 \log p_1 + \dots + x_r \log p_r + \log c$$
Since $p_1 + \dots + p_r = 1$, differentiating $l$ does not work since it's constrained, hence we use the \textbf{Lagrangian} function and treating $p_1, \dots, p_r, \lambda$ as if they are unconstrained
$$\mathcal{L}(p_1, \dots, p_r, \lambda) = x_1 \log p_1 + \dots + x_r \log p_r + \lambda(p_1 + \dots + p_r-  1)$$
\subsubsection{Genetics}
\begin{description}
	\item[Chromosomes] come in pairs, one from each parent
	\item[Locus] a subsequence on a chromosome
	\item[Alleles] different versions of bases at a locus
	\item[Genotype] an unordered pair of alleles
	\begin{itemize}
		\item Given $k$ different alleles, we can construct $k(k+1)/2$ different genotypes
		\item Given the genotype proportions, we can calculate the allele proportions
		\item Given the allele proportions, we can calculate the genotype proportions
	\end{itemize}
\end{description}
\textbf{Mendel's Laws of inheritance}
\begin{itemize}
	\item The maternal allele is randomly chosen from her two alleles; similarly for the paternal allele
	\item The two choices are independent
\end{itemize}
\textbf{Hardy-Weinberg Equilibrium}: A population is in HWE at a locus if the genotype proportions are
$$
f(a_ia_j) =
\left\{\begin{matrix}
p_i^2 & i = j\\
2p_ip_j & i\neq j
\end{matrix}\right.
$$
where $p_i$ is the proportion of allele $a_i$ (assumption: random mating, no mutation, no migration)

\subsection{Large-Sample Variance of ML Estimator}
Let $X$ have density $f(x|\theta)$, $\theta\in\Theta\subset \mathbb{R}^p$. The Fisher information is the $p \times p$ matrix
$$\mathcal{I}(\theta) = -\text{E}\left[\frac{d^2\log{f(X)}}{d\theta^2}\right]$$
with $(i,j)$ entry
$$-\text{E}\left[\frac{\partial^2\log f(X)}{\partial\theta_i\partial\theta_j}\right]$$
$$=-\int_{-\infty}^{\infty}\frac{\partial^2\log f(x)}{\partial\theta_i\partial\theta_j}f(x)dx \text{ or }-\sum_{x}\frac{\partial^2\log f(x)}{\partial\theta_i\partial\theta_j}f(x)dx$$
As $n\rightarrow \infty$,
$$\text{var}(\hat{\theta}_n)\approx \frac{\mathcal{I}(\theta)^{-1}}{n}$$
\subsubsection{Joint Density}
IID $X_1, \dots , X_n$ with density $f(x|\theta)$ can be regarded as a sample from $\textbf{X}=(X_1,\dots , X_n) \in \mathbb{R}^n$ with joint density
$$g(\textbf{X}|\theta) = f(X_1|\theta) \cdots f(X_n|\theta)$$
The information in $\textbf{X}$ is
$$--\text{E}\left[\frac{d^2\log{g(\textbf{X})}}{d\theta^2}\right] = n\mathcal{I}(\theta)$$
where $\mathcal{I}(\theta)$ is the information in any one of the $X$'s
\subsubsection{Multinomial}
Let $\textbf{X}\sim\text{Multinomial}(n,\textbf{p}(\theta))$ where
$$\textbf{p}(\theta) = (p_1(\theta), \dots, p_r(\theta))$$
$$\theta \in \Theta \subset \mathbb{R}^k, 1\leq, k\leq r-1$$
Then,
$$\log f(\textbf{X}) = \sum_{i=1}^{r}X_i\log{p_i}$$
$(i,j)$ entry of $\mathbb{I}(\theta)$:
\begin{equation*}
\begin{aligned}
\frac{n}{p_i} + \frac{n}{p_r}&, i=j\\
\frac{n}{p_r}&, i\neq j
\end{aligned}
\end{equation*}
\subsection{Distribution of MLE}
As $n\rightarrow\infty$, the distribution of
$$\sqrt{n\mathcal{I}(\theta)}(\hat{\theta}_n-\theta)$$ converges to $\mathcal{N}(\textbf{0}, \textbf{I}_p)$.\\
For large $n$,
$$\hat{\theta}_n \sim \mathcal{N}\left(\theta, \frac{\mathcal{I}(\theta)^{-1}}{n}\right)$$
$$1-\alpha \approx \Pr\left(-z_{\frac{\alpha}{2}}\leq \frac{\hat{\theta}_n - \theta}{\sqrt{\mathcal{I}(\theta)^{-1}/n}}\leq z_{\frac{\alpha}{2}}\right)$$
$$1-\alpha \approx \Pr\left( \hat{\theta}_n - z_{\frac{\alpha}{2}}\sqrt{\frac{\mathcal{I}(\theta)^{-1}}{n}}\leq \theta \leq \hat{\theta}_n + z_{\frac{\alpha}{2}}\sqrt{\frac{\mathcal{I}(\theta)^{-1}}{n}} \right)$$
$$1-\alpha \approx \Pr(\text{estimate}-z_{\frac{\alpha}{2}}\text{SE} \leq \theta \leq \text{estimate}+z_{\frac{\alpha}{2}}\text{SE} )$$
Where estimate is drawn using MLE from data, and SE is drawn using the estimate and Fischer information
\subsubsection{Asymptotic Normality}
Let $\hat{\theta}$ be the ML estimator of $\theta$.
\begin{itemize}
	\item For any strictly decreasing/increasing function $h:\Theta \rightarrow \mathbb{R}$, $h(\hat{\theta})$ is also the ML estimator of $h(\theta)$.
	\item For large $n$, $h(\hat{\theta})$ is approximately normal (\textbf{asymptotically normal})
\end{itemize}

\subsection{ML vs MOM}
\begin{itemize}
	\item Both ML and MOM are \textbf{consistent}: bias goes to 0 as $n\rightarrow \infty$
	\item ML is better (smaller bias and SE) because it uses all info contained in the density function, whereas MOM uses only sample moments to estimate parameters
	\item ML estimators have \textbf{asymptotic properties}: as $n\rightarrow \infty$, SE can be estimated without Monte Carlo and so a good CI for the parameter is available
	\item MOM estimators may not be asymptotically normal so it is more difficult to construct a CI, but it is easier to compute so is sometimes useful
\end{itemize}

\section{Goodness-of-fit}
\subsection{Pearson's $X^2$ Test}
Let $(X_1, \dots, X_r) \sim \text{Multinomial(n,\textbf{p})}$ with $n$, $r$ fixed. Then the set of all possible distributions of \textbf{p} is:
$$\Omega = \left\{(p_1,\dots,p_r):p_i>0, \sum_{i=1}^rp_i=1\right\}$$
Consider a subset $\Omega_0$ where \textbf{p} depends on $\theta \in \Theta \subset\mathbb{R}^k, k,r-1$
$$\Omega_0 = \{(p_1(\theta), \dots, p_r(\theta)):\theta\in\Theta\}$$
Now we want to judge if $\textbf{p}\in\Omega_0$ given realisations $(x_1,\dots,x_r)$ (in other words, is \textbf{p} a function which takes in a $k$-dimensional vector $\theta$)
\begin{itemize}
	\item Assuming $(X_1, \dots, X_r) \sim \text{Multinomial}(n,\textbf{p}(\theta)), \theta\in\Theta\subset\mathbb{R}^{k},k<r-1$
	\item $\hat{\theta}$ is the ML estimator of $\theta$
	\item $n\textbf{p}(\hat{\theta})$ is the random expected counts
	\item \textbf{Chi-square statistic}:
	$$X^2 = \sum_{i=1}^r\frac{(X_i-np_i(\hat{\theta}))^2}{np_i(\hat{\theta})} = \sum\frac{(O-E)^2}{E}$$
	\textbf{[Theorem]} As $n\rightarrow\infty$, the distribution of $X^2$ converges to $\chi^2_{r-1-k}$\\
	Note that \underline{$k$ can be 0}, in the case of assuming fair die where there is no parameter to estimate (when the properties are equal)
\end{itemize}
\underline{Steps for $X^2$ goodness-of-fit test}
\begin{enumerate}
	\item Let $H_0 :\textbf{p} \in \Omega_0$
	\item Let $H_1 :\textbf{p} \in \Omega_1$
	\item Substituting each $X_i$ by $x_i$ (the observed realisations) and $\hat{\theta}$ by the ML estimate (to get the expected counts), we get a realisation $x^2$ of $X^2$
	\item The $P$-value: (Calculated assuming $H_0$)
	$$\Pr(X^2\geq x^2) \approx \Pr(\chi^2_{r-1-k}\geq x^2)$$
	The smaller it is, the more suspicious we are of $H_0$ (more likely to reject $H_0$)\\
	The bigger it is, we are more likely to accept $H_0$\\
\end{enumerate}
\subsection{Likelihood Ratio}
Assuming multinomial, Maximum of likelihood $L(\textbf{p}) = \prod_{i=1}^{r}p_i^{X_i}$ over $\Omega$, happens when there is no restriction, do MLE as usual
$$L_1 = L(\hat{p}) = \prod_{i=1}^{r}\left(\frac{X_i}{n}\right)^{X_i}$$
Maximum of likelihood $L(\theta) = \prod_{i=1}^{r}p_i(\theta)^{X_i}$ over $\Omega_0$
$$L_0 = L(\hat{\theta}) = \prod_{i=1}^{r} p_i(\hat{\theta})^{X_i}$$
Note that $L_0/L_1 \geq1$, the larger the ratio, the more we doubt $H_0$
$$2\log\left(\frac{L_1}{L_0}\right)=G$$
$$G=2\sum_{i=1}^rX_i\log{ \left(\frac{X_i}{np_i(\hat{\theta})}\right) }$$
\subsubsection{LR goodness-of-fit test}
\textbf{Assumptions}
\begin{itemize}
	\item $n$ IID RV's density defined by $\theta \in \Omega$ with $k$ independent parameters
	\item $L_1$: maximum likelihood value over $\Omega$
	\item $L_0$: maximum likelihood value over $\Omega_0$ with $k_0<k_1$ independent parameters
\end{itemize}
\textbf{Theorem}: Suppose $\theta\in\Omega_0$ (Assume $H_0$ is true). As $n\rightarrow\infty$, the distribution of
$$G = 2\log\left(\frac{L_1}{L_0}\right)$$ converges to $\chi^2_{k_1-k_0}$
\textbf{LR goodness-of-fit test}
\begin{enumerate}
	\item $H_0:\theta\in\Omega_0$
	\item $H_1:\theta\in\Omega_1$
	\item $L_0$ and $L_1$ are the maximum likelihood values under $\Omega_0$ and $\Omega_1$
	$$g=2\log\left(\frac{L_0}{L_1}\right)$$
	is a realisation of $G$
	\item The $P$-value is calculated with distribution of $G$ under $H_0$
	$$\Pr(G\geq g) \approx \Pr(\chi^2_{k_1-k_0}\geq g)$$
\end{enumerate}



\subsection{Poisson Dispersion Test}
For Poisson, if var is more or less the same as mean, then it can fit well. But if var is $\gg$ mean then need to find new distribution or the data might come from two or more different RV's
\begin{enumerate}
	\item $H_1:\theta\in\Omega$: For $i=1,\dots,n$, $X_i\sim\text{Poisson}(\lambda_i)$ and each are independent
	$$l(\lambda_1,\dots,\lambda_n)=\sum_{i=1}^{n}X_i\log{\lambda_i}-\sum_{i=1}^n\lambda_i$$
	When $l(\lambda_1,\dots,\lambda_n)$ is maximum, $\hat{\lambda} = X_i$, so maximum likelihood under $\Omega:l_1=\sum_{i=1}^nX_i\log X_i - \sum_{i=1}^nX_i$
	\item $H_0:\theta\in\Omega_0$: Every $\lambda_i=\lambda$
	$$l(\lambda) = \sum_{i=1}^{n}X_i\log\lambda-n\lambda$$
	Maximum likelihood is achieved when $\hat{\lambda}=\bar{X}$ under $\Omega_0:l_0 =\sum_{i=1}^nX_i\log\bar{X}-n\bar{X}$
	\item Calculate $P$-value $$G=2\sum_{i=1}^{n}X_i\log\left(\frac{X_i}{\bar{X}}\right)\approx \frac{\sum_{i=1}^n(X_i-\bar{X})^2}{\bar{X}}$$
	Suppose every $\lambda_i=\lambda$. For large $n$, $G\sim\chi_{n-1}^2$ approximately
\end{enumerate}






\section{Useful Results}
%\vspace{1em}

%\vspace{1em}
\subsection{Algebra}
$$\sum_{i=1}^n(X_i-\bar{X})^2 = \sum_{i=1}^nX_i^2 - n\bar{X}^2$$
$$\hat{\theta}_n\sim\mathcal{N}\left(\theta, \frac{\mathcal{I}(\theta)^{-1}}{n}\right)$$
\subsection{Procedures}
\fbox{\begin{minipage}{22em}
		\textbf{Framework for statistical inference}:
		\begin{enumerate}
			\item Parameter is a simple function of the population, real or hypothetical
			\item Data are realisations of IID RV's (if $n \ll N$)
			\item Estimate is a realisation of an estimator, whose SD is the SE. For large $n$, can construct CI.
			\item $\text{MSE} = \text{SE}^2 + \text{bias}^2$
		\end{enumerate}
\end{minipage}}

\subsection{Multivariable Calculus}
\begin{itemize}
	\item Use Hessian matrix to calculate partial derivatives/maximum points, and $|H| > 0$
\end{itemize}
\end{multicols}






\end{document}