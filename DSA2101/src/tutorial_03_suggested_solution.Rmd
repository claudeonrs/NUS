---
title: "Tutorial 3 Worksheet AY 22/23 Sem 1"
subtitle: DSA2101
output:
  pdf_document: default
  html_document: default
url_color: blue
---

## Introduction to the dataset

The data for this week comes from the 
[UCI machine learning repository](https://archive.ics.uci.edu/ml/datasets/Grammatical+Facial+Expressions). 

Unzip the file `grammatical_facial_expression.zip` and place it in your `data` 
folder. On my local computer, I now see the following structure:
```{r}
list.files("../data/grammatical_facial_expression/")[1:3]
```

```{r echo=FALSE}
read.fe_data <- function(path_to_dir, user, facial_expression) {
  all_facial_exp <- c("affirmative", "conditional", "doubts_question", 
                      "emphasis",  "negative",  "relative",
                      "topics", "wh_question", "yn_question")
  facial_expression <- match.arg(facial_expression, all_facial_exp)
  
  fnames <- paste(paste(user, facial_expression, 
                        c("datapoints", "targets"), sep="_"), ".txt", sep="")
  output_vec <- scan(file.path(path_to_dir, fnames[1]), skip = 1)
  binary_class <- scan(file.path(path_to_dir, fnames[2]))
  l_o_vec <- length(output_vec)
  output_array <- array(output_vec[-seq(1, l_o_vec, by=301)], 
                        dim=c(3, 100, length(binary_class)))
  output_array <- aperm(output_array, c(2,1,3))
  
  
  tmp_df <- data.frame(timestamp= output_vec[seq(1, l_o_vec, by=301)],
                       user = user, facial_expression=facial_expression, 
                       fe_present=binary_class)
  output_obj <- list(frames=output_array, info=tmp_df)
  class(output_obj) <- "gfe"
  output_obj
}

plot.gfe <- function(x, id=1, ...) {
  tmp_frame <- x$frames[,,id,drop=TRUE]
  plot(-tmp_frame[,1], -tmp_frame[,2], asp = 1, 
       type="n", xaxt='n', yaxt='n', xlab="", ylab="", bty="n")

# 0 - 7	(x,y,z) - left eye
# 8 - 15 	(x,y,z) - right eye
# 16 - 25	(x,y,z) - left eyebrow
# 26 - 35	(x,y,z) - right eyebrow
# 36 - 47	(x,y,z) - nose
# 48 - 67	(x,y,z) - mouth
# 68 - 86	(x,y,z) - face contour
# 87		(x,y,z) - left iris
# 88 		(x,y,z) - right iris
# 89		(x,y,z) - nose tip
# 90 - 94	(x,y,z) - line above left eyebrow
# 95 - 99	(x,y,z) - line above right eyebrow
  
  face_features <- list(left_eye = c(1:8,1),
                        right_eye = c(9:16, 9),
                        left_brow = c(17:26, 17),
                        right_brow = c(27:36, 27),
                        nose = 37:48,
                        mouth = c(49:60, 49),
                        lips= c(61:68, 61),
                        face_contour = 69:87,
                        #irises_nose_tip = 88:90)
                        left_line =91:95, 
                        right_line = 96:100)
  lapply(face_features, 
         function(x) lines(-tmp_frame[x,1], -tmp_frame[x,2], col="red"))
  points(-tmp_frame[88:90, 1:2], col="red")
}
```

The data comes from video images of facial expressions when using Brazilian sign
language. Facial expressions are an essential component of sign language, since
they modify the meaning of the sign. These facial expressions are known as
Grammatical Facial Expressions (GFEs). This dataset was collected to see if it
is possible to train a classifier to identify the presence of a GFE from the
facial image of the signer. There are 9 GFEs considered: 

* `wh_questions`: who/what/when/where/how questions
* `yes/no questions:` yes/no questions
* `doubt questions`: doubt questions
* `topics`
* `assertions`
* `negatives`
* `conditional clauses`
* `focus`
* `relative clauses`

The videos were captured by a Microsoft Kinect sensor. The videos come from two
users (`a` and `b`). In each video, a user performs (five times), in front of
the sensor, five sentences in Libras (Brazilian Sign Language) that require the
use of a grammatical facial expression. Using Microsoft Kinect, they have 
obtained: 

* a text file (*_datapoints.txt) containing one hundred coordinates (x, y, z) of
points from eyes, nose, eyebrows, face contour and iris; each line in the file
corresponds to points extracted from one frame. The z-coordinate is depth from
the sensor. The first value in each line is a timestamp for that frame in the video.
* a corresponding text (*_targets.txt) file with a manually labelled
classification of whether there is a GFE or not. Each line in this file corresponds
to a frame in the datapoints.txt file.

```{r echo=FALSE, fig.align="center", out.width="60%"}
#knitr::include_graphics("../figs/gfe.png")
```

The dataset is organized in 36 files: 18 datapoint files and 18 target files,
one pair for each video which compose the dataset.The name of the file refers to
each video: the letter corresponding to the user (a and b), name of grammatical
facial expression and a specification (target or datapoints).

## Questions 

1. Use `scan` to read in one of the datapoints files. What does it return?
```{r message=FALSE}
tmp_frames<- scan("../data/grammatical_facial_expression/a_negative_datapoints.txt",
                skip=1)
```
2. We would like to represent the coordinates in each frame as a matrix with 
   dimension 100x3. We would then like to represent the information in each 
   datapoints file as a multidimensional array (100 x 3 x n) where n is the 
   number of frames. Here is an *example*:
```{r message=FALSE}
tmp_classes <- scan("../data/grammatical_facial_expression/a_negative_targets.txt")
l_class_vec <- length(tmp_classes) # no. of frames
output_array <- array(tmp_frames[-seq(1, length(tmp_frames), by=301)],  
                      dim=c(3, 100, length(tmp_classes)))
output_array <- aperm(output_array, c(2,1,3))
```

   `output_array` is now of dimension 100x3x1124. We can access the coordinates 
   in the first frame with `output_array[ , , 1]`, and so on.

3. Write a function `read.fe_data` that works in the following way, and returns 
   an object of class `gfe`. You can find an example of such an object in 
   `example_gfe.rds`.

```{r}
args(read.fe_data)
```

```{r}
obj1 <- read.fe_data("../data/grammatical_facial_expression", user="b", 
                     "emphasis")
str(obj1)
```

2. The 100 coordinates for each frame correspond to the following facial 
   features:
   
   * 0 - 7	(x,y,z) - left eye
   * 8 - 15 	(x,y,z) - right eye
   * 16 - 25	(x,y,z) - left eyebrow
   * 26 - 35	(x,y,z) - right eyebrow
   * 36 - 47	(x,y,z) - nose
   * 48 - 67	(x,y,z) - mouth
   * 68 - 86	(x,y,z) - face contour
   * 87		(x,y,z) - left iris
   * 88 		(x,y,z) - right iris
   * 89		(x,y,z) - nose tip
   * 90 - 94	(x,y,z) - line above left eyebrow
   * 95 - 99	(x,y,z) - line above right eyebrow
   
   Write a plot method for objects of class `gfe`, that works in the following 
   way. The `id` argument selects and plots the id-th frame in the `gfe` object.

```{r fig.align='center'}
plot(obj1, id=1000)
```

## Comments 

The primary learning points of this tutorial are to:

* know how to assign an S3 class (e.g. gfe)
  * use `class(obj) <- "gfe"`
* know how to write a specific method for a class:
  * In this case, we created `plot.gfe`, which we will call as `plot(gfe_obj)`.
* remember that, when we are given data, it is our duty to ensure it makes sense. If 
  our data is non-conventional, we should write utility/convenience functions 
  to explore it quickly.
* understand that R has capacity to handle multi-dimensional data structures (arrays). We 
  also use the `[` notation to access individual slices of the data.

## Hints

1. If you need more information about the data, do refer to the file `data_descriptions.txt`.
2. The `gfe` object contains a multi-dimensional array. It can be created with `array`.
   Remember that R is column-major, so store your coordinates in the correct way.
3. Code like this week is not for presenting to others, but for us to work with our 
   data efficiently. It is worth taking the time to write it well, because we might use 
   it frequently, and so might our colleagues. What other functions might be useful 
   when we are exploring our data?
4. When writing up the function, practice on one of the frames first. Understand the
   data completely before packaging it into a function.

