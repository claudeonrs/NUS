knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(jsonlite)
library(stringr)
rest_json <- stream_in(file("../data/restaurants_dataset.json")) %>%
unnest(cols=c(address,grades))
q1_tbl <- mutate(rest_json,
lat=sapply(coord, function(x) x[2]),
long=sapply(coord, function(x) x[1]),
.before=street) %>%
select(!coord)
range(q1_tbl$lat, na.rm=TRUE)
range(q1_tbl$long, na.rm=TRUE)
mistakes <- q1_tbl %>% filter(lat < 0 & long > 0)
summarise(group_by(q1_tbl, name), neg_lat=any(lat < 0), pos_long=any(long > 0))
mistakes2 <- q1_tbl %>%
group_by(restaurant_id) %>%
summarise(false_lat = any(lat <0), false_long = any(long > 0)) %>%
filter(false_long == TRUE | false_lat == TRUE)
american_manhattan <- q1_tbl %>%
filter(borough == "Manhattan" & str_detect(cuisine, "[Aa]merican")) %>%
group_by(restaurant_id) %>%
summarise(name = max(name))
# Number of restaurants in Manhattan that serve American
dim(american_manhattan)[1]
grading <- q1_tbl %>%
group_by(restaurant_id) %>%
summarise(name=max(name), n=n()) %>%
arrange(desc(n))
shortest_time <- q1_tbl %>%
group_by(restaurant_id) %>%
filter(n() > 1) %>%
arrange(desc(date)) %>%
mutate(interval = abs(difftime(date,lag(date)))) %>%
summarise(name=max(name), shortest_interval = min(interval, na.rm=TRUE))
shortest_time
shortest_time %>% arrange(shortest_interval)
knitr::opts_chunk$set(echo = TRUE)
library(stringr)
box_muller <- function(n) {
# function to generate two random variables
box_muller2 <- function(i) {
u <- runif(2, min=0, max=1);
x1 <- sqrt(-2*log(u[1]))*cos(2*pi*u[2])
x2 <- sqrt(-2*log(u[1]))*sin(2*pi*u[2])
c(x1,x2)
}
# apply n/2 or (n+1)/2 times and return the result as a vector
apply_times <- ifelse(n%%2==0, n/2, (n+1)/2)
c(sapply(1:apply_times,box_muller2))[1:n]
}
read.fe_data <- function(path_to_dir, user, facial_expression) {
# @param string path_to_dir
# @param string user
# @param string facial_expression
files <- list.files(path_to_dir, full.names=TRUE)
fnames <- str_subset(files, str_c(user,"_",facial_expression))
dp_path <- str_subset(fnames, "datapoint")
cat(dp_path)
tg_path <- str_subset(fnames, "target")
dp_vec <- scan(dp_path, skip=1)
tg_vec <- scan(tg_path)
total_frames <- length(tg_vec)
frames <- array(dp_vec[-seq(1, length(dp_vec), by=301)],
dim=c(3, 100, total_frames))
frames <- aperm(frames, c(2,1,3))
timestamp <- dp_vec[seq(1, length(dp_vec), by=301)]
user <- rep(user, length.out=total_frames)
facial_expression <- rep(facial_expression, length.out=total_frames)
fe_present <- tg_vec
df <- data.frame(timestamp, user, facial_expression, fe_present)
out <- list(frames=frames,info=df)
class(out) <- "gfe"
out
}
gfe_obj <- read.fe_data("../data/grammatical_facial_expression",
"b","emphasis")
str(gfe_obj)
`[.gfe` <- function(gfe_obj, vec) {
out <- gfe_obj
out$frames <- gfe_obj$frames[,,vec]
out$info <- gfe_obj$info[vec,]
out
}
sub_gfe_obj <- gfe_obj[1:3]
str(sub_gfe_obj)
str(gfe_obj[-1:-2])
str(gfe_obj[1:3])
str(gfe_obj[1:10])
str(gfe_obj[1:100])
str(gfe_obj[c(TRUE,FALSE)])
sub_gfe_obj1 <- gfe_obj[1:3]
str(sub_gfe_obj1)
sub_gfe_obj2 <- gfe_obj[-1:-3]
str(sub_gfe_obj2)
sub_gfe_obj3 <- gfe_obj[c(TRUE,FALSE)]
str(sub_gfe_obj3)
x <- rnorm(1000000)
mat <- matrix(x, nrow=10000)
head(mat)
?apply
apply(mat, 1, function(x) {})
apply(mat, 1, function(x) {mean <- mean(x); se <- sd(x)/sqrt(length(x)); c(mean-se, mean+se)})
ci <- apply(mat, 1, function(x) {mean <- mean(x); se <- sd(x)/sqrt(length(x)); c(mean-se, mean+se)})
head(ci)
ci[1]
ci[1,2]
ci[1,1]
ci[1,]
dim(ci)
dim(x)
dim(mat)
estimate <- apply(ci, 2, function(x) {x[1] <= 0 & x[2] >= 0})
estimate
mean(estimate)
pnorm(1)
qnorm(1)
dnorm(1)
pnorm(1)-pnorm(-1)
